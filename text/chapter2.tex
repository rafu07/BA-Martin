\section{Grundlagen und Begriffserklärungen}

\subsection{Die Composable-Enterprise-Architektur}

\subsubsection{Begriffserklärung und Abgrenzung}
\label{sec:CEA_B}
Nach Ansicht von Steve Denning, Managementberater und Autor der Forbes, stellen Flexibilität, Resilienz und Agilität wesentliche Faktoren dar, welche zur Steigerung der Wettbewerbsfähigkeit von Unternehmen beitragen \cite{Denning.20170210}. Für Analystenhäuser wie Gartner steht fest, dass es technologischer Innovation bedarf, um einhergehende Herausforderungen erfolgreich zu bewältigen und eine kontinuierliche Unternehmenstransformation voranzutreiben. Gartner empfiehlt dabei monolithische und starre Softwarearchitekturen, durch einen modularen Systemaufbau zu ersetzen. In seinen Veröffentlichungen verwendet Gartner für dieses Konzept den Begriff der \textbf{\ac{CEA}}.
\begin{center}
	\begin{figure}[H]
		\centering
		\scalebox{0.3}{\includegraphics{CEA}}
		\caption[Entstehung einer Composable-Enterprise-Architektur]{Entstehung einer Composable-Enterprise-Architektur. In Anlehnung an Schönstein \cite{Schonenstein.20230103}.}
		\label{fig:CEA_S}
	\end{figure}	
\end{center}
\vspace*{-15mm}
In der Literatur wird die CEA dabei wie folgt definiert:\vspace{2mm}\\
\textit{\enquote{[The Composable-Enterprise-Architecture (CEA)] is an approach to designing and implementing enterprise architectures focused on flexibility, scalability, and adaptability. This is done through the combination of packaged business capabilities \cite{Gartner.20230418}\cite{MuleSoft.20230427}.}}\vspace{2mm}\\
Eine CEA beschreibt somit ein aus mehreren Bausteinen, sog. \textit{\ac{PBC}} bestehenden Systemaufbau. PBCs sind vorgefertigte Softwareelemente, welche jeweils eine bestimmte Geschäftsfunktion abdecken (s. Abb. \ref{fig:CEA_S}). Das Konzept der PBCs basiert dabei auf den drei Prinzipien der CEA: \textit{Modulare Architektur}, \textit{Offenes Ökosystem} und \textit{Business-Zentriertheit} \cite{.20230313}.\\ Laut Gartner müssen Unternehmen nicht nur \enquote{akzeptieren, dass der disruptive Wandel zur Normalität gehört}. Vielmehr sollten diese den disruptiven Wandel als \enquote{Chance begreifen und ihn nutzen, um eine \textit{modulare Architektur} zu implementieren} \cite{.20230313}. In diesem Kontext wird von dem Analystenhaus Gartner ebenfalls der Begriff des \textit{Composable-Enterprise-Ressource-Planning-Systems (Composable-\acs{ERP}-Systems)} verwendet. Hierbei stellen die modularen Komponenten (PBCs) vorgefertigte Geschäftsfunktio\-nen- bzw. -prozesse dar, welche als Module in ein Composable-ERP-System integriert werden können. Diese PBCs können etwa Funktionen für Finanzbuchhaltung, Einkauf, Verkauf, Lagerverwaltung, Produktion oder Personalmanagement enthalten. Ergibt sich eine Änderung in den Geschäftsanforderungen, ermöglicht diese Architektur ein flexibles und isoliertes Austauschen, Verändern sowie Weiterentwickeln einzelner PBCs \cite{Gartner.20230418}. Durch die unabhängige Bereitstellung der Softwarekomponenten ist es weiterhin möglich, einzelne Module einer CEA skalierbar zu gestalten, ohne dabei die Gesamt-Suite anpassen zu müssen. Somit sind Unternehmen in der Lage, die Rechenkapazität einzelner ERP-Module flexibel und kosteneffizient an wachsende Geschäftsanforderungen anzupassen \cite[7]{Sensedia.2020}.\\ Ein Composable-ERP-System besteht aus einer Kollektion von Kernkomponenten (s. Abb. \ref{fig:CERP}). Diese Kernkomponenten werden i.d.R. von einem einzigen ERP-Anbieter bereitgestellt und können deshalb ohne hohen Aufwand integriert und aufeinander abgestimmt werden \cite{.d}. Bei diesen Komponenten handelt es sich dabei um Funktionalitäten, welche das Hauptgeschäft eines Unternehmens unterstützen. Um den sich ändernden Geschäftsanforderungen gerecht zu werden, können diese Kernkomponenten durch zusätzliche PBCs erweitert werden. Dieses Konzept basiert auf dem Prinzip des \textit{offenen Ökosystems}. Die externen, in das Kern-ERP-System integrierbaren PBCs, umfassen dabei i.d.R. stark spezialisierte Funktionalitäten. So könnte sich ein E-Commerce-Unternehmen dazu entschließen, eine Kern-\acl{CRM}-Komponente (Kern-CRM-Komponenten) um eine auf Kün\-stlicher Intelligenz (\acs{KI}) basierenden Kundenanalysefunktion zu erweitern. Um die Integration dieser modularen Komponenten zu erleichtern, stellen ERP-Anbieter auf ihren Plattformen einen zur Bündelung der PBCs verwendeten Marktplatz zur Verfügung \cite{.d}. Die dabei angeboten Komponenten können zusätzliche Funktionen des ERP-Kern\-anbieters oder externe Komponenten von Spezialherstellern darstellen. Auf diese Weise haben Mitarbeiter oder Teams die Möglichkeit, innerhalb ihres Unternehmens auf diesen Marktplatz zuzugreifen und PBCs, welche zur Unterstützung der operativen Tätigkeiten benötigt werden, ohne hohen Aufwand zu aktivieren. Das ERP-System kann somit auf die spezifischen Bedürfnisse und Anforderungen der Unternehmen zugeschnitten werden \cite{.20230313}.
\begin{center}
	\begin{figure}[H]
		\centering
		\scalebox{0.40}{\includegraphics{CERP}}
		\caption[Bestandteile eines Composable-ERP-Systems]{Bestandteile eines Composable-ERP-Systems. Eigene Darstellung.}
		\label{fig:CERP}
	\end{figure}	
\end{center}
\vspace*{-15mm}
IT-Systeme sind primär auf eine Unterstützung operativer Tätigkeiten ausgerichtet. Um eine anwenderzentrierte Gestaltung der auf dem Marktplatz angebotenen Werkzeuge zu ermöglichen, sollte der Fokus dieser Tools auf den Bedürfnissen und Erwartungen der Anwender liegen (\textit{Business-Zentriertheit}). Deshalb ist essenziell, dass Mitarbeiter PBCs intuitiv nutzen und ggf. weiterentwickeln sowie anpassen können, ohne dabei von IT-Abteilungen abhängig zu sein \cite{.20230313}. Dabei soll die Verwendung von Low-Code/No-Code-Plattformen Abhilfe schaffen. Diese ermöglichen den Mitarbeitern eigene PBCs zu entwickeln ohne auf spezielle IT-Kenntnisse oder -Ressourcen angewiesen zu sein. Damit wird die Agilität und Flexibilität der CEs erhöht, während die Abhängigkeit von IT-Abteilungen reduziert wird.

\subsubsection{Technologische Konzepte des Composable-Enterprises}
Um die in Kapitel \ref{sec:CEA_B} aufgeführten Grundsätze der CEA in das Unternehmen zu integrieren, benötigt es verschiedener technologischer Konzepte. Zusammenfassen lassen sich diese mit dem Akronym \textit{MACH}: \textit{\acl{MACH}}.
\begin{center}
	\begin{figure}[H]
		\centering
		\scalebox{0.3}{\includegraphics{CEA_K}}
		\caption[Technische Realisierung der Composable-Enterprise-Architektur]{Technische Realisierung der Composable-Enterprise-Archi\-tektur. Eigene Darstellung.}
		\label{fig:CEA_K}
	\end{figure}	
\end{center}
\vspace*{-15mm}
Der zentrale Einstiegspunkt in eine CE-Anwendung ist das User-Interface. Für die Frontend-Entwicklungen wird dabei i.d.R. das \textit{Headless-Konzept} verwendet (s. Abb. \ref{fig:CEA_K}). Dieses beschreibt, dass zwischen Front- und Backend keine feste Kopplung besteht. Vielmehr werden auf dem Backend standardisierte Daten verwaltet, welche auf verschiedenen Frontends ausgegeben werden können \cite[4]{Attardi.2020b}. Dies gewährleistet eine individuelle Gestaltung der Benutzeroberfläche über verschiedene Kanäle und Plattformen (z. B. Web, Mobile) hinweg.\\ Die Geschäftsfunktionen einer CEA werden in verschiedene PBCs gekapselt. In der Applikationslogik des Backends kann ein PBC durch ein oder mehrere \textit{Microservices} dargestellt werden. Ein Microservice ist eine eigenständige Einheit, welche für eine spezifische Funktion zuständig ist \cite[S. 93 ff.]{Lauretis.1027201910302019}. Im Kontext eines E-Commerce-Unternehmens könnte ein PBC etwa eine Bestellverwaltung sein. Diese Bestellverwaltung kann dabei aus verschiedenen Microservices wie einem Bestellannahme-, Auftragsabwicklungs- oder Rechnungsstellungsdienst bestehen. Microservices spielen dabei hauptsächlich für die technische Realisierung eine Rolle. Folglich sind diese für den Endanwender nicht erkennbar. Es ist möglich, für jeden Service eine unterschiedliche Programmiersprache sowie Datenbank zu verwenden. So können Architektur und Technologien eines Dienstes unmittelbar an dessen betriebswirtschaftliche Anforderungen angepasst werden \cite[42]{Balalaie.2016}.\\ Zur Kommunikation zwischen Services werden standardisierte Schnittstellen, sog. \textit{\ac*{APIs}} verwendet. Mit APIs werden die von den Services bereitgestellten Funktionalitäten und Daten veröffentlicht \cite[15]{Biehl.2015}. Diese Schnittstellen können dabei unmittelbar von dem Frontend oder anderen Microservices konsumiert werden. Ein weiteres in CEs verwendetes Kommunikationskonzept ist die \textit{\ac{EDA}}. Die EDA ist ein Architekturkonzept, bei welchen Microservices asynchron über eine zentrale Vermittlungsinstanz, dem \textit{Message Broker}, kommunizieren \cite[54]{Bruns.2010}.\\ Alle Komponenten der CEA werden auf einer Cloud-Plattform betrieben (\textit{Cloud-native}) \cite[3]{Kratzke.2017}. Cloud-Computing ist ein Dienstleistungsmodell, welches Nutzern ermöglicht, Ressourcen, wie Speicher, Analyse-Tools oder Software über das Internet von einem Cloud-Anbieter zu beziehen \cite[5]{Reinheimer.2018}. Dieses Computing-Modell ermöglicht IT-Services schnell und kosteneffizient an aktuelle Markterfordernisse anzupassen. Aufgrund der nutzungsabhängigen Bepreisung von Cloud-Plattformen können Dienste ohne hohen Investitionseinsatz auf- und abgebaut werden \cite[10]{Reinheimer.2018}. Für das Cloud-Computing werden durch das \ac{NIST} verschiedene Servicemodelle definiert. Neben \textit{Software-as-a-Ser\-vice (\acs{SaaS})} und \textit{\ac{IaaS}}, bei welchem eine Anwendung bzw. eine gesamte Infrastruktur in der Cloud gemietet wird, gibt es ebenfalls das \textit{\ac{PaaS}} \cite{Reinheimer.2018} \cite[9]{Reinheimer.2018}. Bei diesem Computing-Modell wird eine Plattform bereitgestellt, auf welcher Kunden eigene Dienste entwickeln, testen und betreiben können. Ein auf dieser Service-Ebene von der SAP bereitgestelltes Produkt ist die \ac{SAP BTP}. Diese stellt eine Reihe von Diensten und Funktionen zur Verfügung, mit welchen Unternehmen SAP-ERP-Systeme anpassen, integrieren und erweitern können. Zum Betrieb von IT-Services werden mit der SAP BTP verschiedene Laufzeitumgebung wie das Cloud Foundry ausgeliefert. Cloud Foundry stellt eine abstrahierte Computing-Umgebung dar, welche essenzielle Verwaltungsfunktionalitäten zur Skalierung, Nutzlastverteilung sowie Überwachung von Diensten bündelt \cite{.20230429}. Diese Laufzeitumgebung wird dabei i.d.R. für SAP-CAP-Node- sowie SAP-UI5-Applikationen verwendet. SAP CAP Node ist ein auf Node.js basierendes Programmier-Framework, welches zur Implementierung von Datenmodellen, Geschäftslogiken sowie APIs eines Microservices verwendet wird \cite{Steckenborn.20210526}. Dagegen stellt SAP UI5 ein clientseitiges HTML5-Framework dar, welches eine Vielzahl von Steuerelementen zur Entwicklung von SAP-Benutzeroberflächen bereitstellt \cite{.20210609}. 

\subsection{Integration und Bereitstellung einer Cloud-Anwendung}
\subsubsection{Agile und DevOps als moderne Softwareentwicklungskonzepte}
Um eine maßgeschneiderte Composable-ERP-Lösung bereitzustellen, ist es erforderlich, dass Unternehmen ihre Systemlandschaften um eigenentwickelte Microservices erweitern oder bereits bestehende Komponenten modifizieren. Zur Koordination des dabei resultierenden Entwicklungs- und Bereitstellungsprozesses, werden i.d.R. verschiedene Softwareentwicklungskonzepte verwendet. Dabei gibt es das traditionelle Wasserfallmodell, welches eine sequenzielle Abfolge der Projektelemente \textit{Anforderung}, \textit{Design}, \textit{Implementierung}, \textit{Test} und \textit{Betrieb} vorgibt. Im Hinblick auf die mit einer CEA angestrebten Flexibilität weist dieses Entwicklungskonzept jedoch Restriktionen auf. Die in dieser Methodik detailliert durchgeführte Vorabplanung, kann insbesondere in umfangreichen Langzeitvorhaben aufgrund unvorhersehbarer Externalitäten selten eingehalten werden \cite[5]{Vivenzio.2013}. Dies resultiert nicht nur in einem Anstieg der Kosten, sondern führt ebenfalls dazu, dass IT-Projekte länger als geplant dauern \cite[41]{Vieweg.2015}.\\ Als Reaktion haben sich innerhalb der Projektmanagementlandschaft zunehmend \textbf{agile Vorgehensmodelle} etabliert.
Im Gegensatz zum Wasserfallmodell, welches eine umfassende Vorabplanung vorsieht, wird das Vorhaben in einer agilen Entwicklung in viele zyklische Einheiten, sog. \textit{Sprints}, segmentiert (s. Abb. \ref{fig:Agile_Cycle}) \cite[87]{Goll.2015}. Sprints repräsentieren Durchläufe, welche i.d.R. eine Dauer von ein bis vier Wochen aufweisen. Alle innerhalb des Projektumfangs zu entwickelnden Funktionalitäten werden dabei in einem zentralen Artefakt (\textit{Product Backlog}) festgehalten und von dem Produktverantwortlichen (\textit{Product Owner}) priorisiert \cite[196]{Gloger.2010}. Während eines Sprints ist die Fertigstellung eines vor diesem Abschnitt definierten Aufgabenkontingents (\textit{Sprint Backlog}) vorgesehen \cite[194]{.2013}. Nach Abschluss eines Sprints soll dabei ein potenziell an den Kunden auslieferbares Produkt zur Verfügung gestellt werden. Dies erlaubt eine schnelle Bereitstellung funktionsfähiger Software. Zudem kann das an die Stakeholder ausgelieferte Artefakt als Feedback-Grundlage verwendet und daraus resultierenden Anforderungen im unmittelbaren Folge-Sprint eingearbeitet werden \cite[S. 180 ff.]{Gloger.2016b}.
\begin{center}
	\begin{figure}[H]\hspace*{-5mm}
		\centering
		\scalebox{0.55}{\includegraphics{Agile_Cycle}}
		\caption[Exemplarische Abfolge eines agilen Entwicklungszykluses]{Exemplarische Abfolge eines agilen Entwicklungszykluses. In Anlehnung an K\&C \cite{K&C.2021}.}
		\label{fig:Agile_Cycle}
	\end{figure}	
\end{center}
\vspace*{-15mm}
Innerhalb der letzten Dekade haben sich diverse auf agilen Prinzipien basierenden Vorgehensmodelle, wie Scrum, Kanban oder \ac{XP} in der Softwareentwicklung etabliert. Obwohl einige dieser Methoden zur erfolgreichen Zusammenarbeit innerhalb der Entwicklungsteams beigetragen haben, bleibt das sog.  \textit{Problem der letzten Meile} bestehen \cite{Qentelli.20230305}. Traditionell erfolgt eine funktionale Trennung der Entwickler- und IT-Betriebler-Teams. Das Problem der letzten Meile beschreibt dabei, dass aufgrund ausbleibender Kooperation dieser Teams der Programmcode nicht auf die Produktivumgebung abgestimmt ist \cite[17]{Huttermann.2012}. Erkenntnisse aus der Praxis zeigen, dass solche organisatorischen Silos häufig in einer schlechten Software-Qualität und somit in einem geminderten Ertragspotenzial bzw. in einer Erhöhung der Betriebskosten resultieren \cite[1]{Halstenberg.2020}. So geht aus der von Avasant veröffentlichten Studie \textit{IT-Spending And Staffing Benchmarks} hervor, dass durchschnittlich 75 Prozent des Unternehmens-IT-Budgets zur Erhaltung des Status quo, also zum Betrieb bestehender Anwendungen verwendet wird. Stattdessen fordert das Beratungshaus eine Rationalisierung der Bereitstellung von Software, um finanzielle Mittel für wertschöpfende Investitionen zu maximieren \cite[6]{Avasant.20220728}.\\ Abhilfe schaffen kann das in der Literatur als \textbf{\acl{DevOps} (Dev\-Ops)} bekannte Aufbrechen organisatorischer Silos zwischen Entwicklung und dem IT-Betrieb \cite[1]{Halstenberg.2020}. 
Dabei stellt DevOps keine neue Erfindung dar. Stattdessen werden einzelne bereits bewährte Werkzeuge, Praktiken und Methoden wie z.B. die agile Softwareentwicklung zu einem umfassenden Rahmenwerk konsolidiert. Prägnant zusammenfassen lässt sich das DevOps-Konzept durch das Akronym CAM: \textit{Culture (Kultur)}, \textit{Automation (Automatisierung)} und \textit{Measurement (Messung)} \cite[5]{Halstenberg.2020}. Dabei gilt \textit{Kultur} als das wohl wesentlichste DevOps-Erfolgselement. Diese bezweckt eine Kollaborationsmentalität, welche sich über alle Ebenen eines Unternehmens erstreckt. Operative Entscheidungen sollen dabei auf die Fachebenen herunter delegiert werden, welche aufgrund ihrer spezifischen Expertise am geeignetsten sind, Dispositionen zu verabschieden \cite[5]{Halstenberg.2020}. Eine \textit{Automatisierung} der Softwarebereitstellungsprozesse ermöglicht, sich wiederholende manuelle Arbeit zu eliminieren. Dies kann ebenfalls zur Rationalisierung und damit zur Senkung der IT-Betriebskosten beitragen. Der dabei erzielte Einfluss wird anhand verschiedener DevOps-Kennzahlen bemessen (\textit{Messung}). Neben der Systemverfügbarkeit und der Instandsetzungszeit ist für Softwareentwicklungsunternehmen in diesem Kontext insbesondere das \textit{Time-to-Market (\acs{TTM})} signifikant \cite[7]{Halstenberg.2020}. Dieser beschreibt die Zeitspanne zwischen Entwicklungsentstehungsprozess und der Markteinführung von IT-Services \cite[141]{Vesey.1992}. Dabei soll die mit DevOps angestrebte Verschmelzung der Entwicklungs- und Betriebsteams, die Automatisierung von Prozessen sowie die kontinuierliche Integration und Bereitstellung von IT-Services zu einer erheblichen Reduzierung dieser Metrik führen. Martin Fowler, Chief Scientist des Softwaredesignunternehmens ThoughtWorks, sieht in einer schnellen Bereitstellung von IT-Services angesichts der bei Softwareintegration entstehenden Komplexität, einen erheblichen Vorteil. So können Anwendungen schnellst möglichst in produktionsähnlichen Umgebungen getestet und sukzessive mit dem Kunden verbessert werden \cite{.20230407}. 

\subsubsection{Automatisierung der Integrations- und Bereitstellungsprozesse}
\label{sec:CICD}
 Ein integraler Bestandteil des DevOps-Rahmen\-werks ist \textit{\ac{CI/CD}}. CI/CD ist ein Verfahren, welches zur Verbesserung der Qualität bzw. zur Senkung der Entwicklungszeit von IT-Services beitragen soll. Abhilfe schaffen soll dabei eine sog. CI/CD-Pipeline, welche alle Schritte von Integration des Codes  bis Bereitstellung der Software automatisiert. Hauptaugenmerk liegt dabei auf einer zuverlässigen und kontinuierlichen Bereitstellung von Software \cite[471]{Zampetti.92720211012021}. Alle in diesem Prozess anfallenden Aktivitäten werden im CI/CD-Zyklus der Abb. \ref{fig:CICD_Cycle} dargestellt. 
 \begin{center}
	\begin{figure}[H]
		\centering
		\scalebox{0.5}{\includegraphics{CICD_Cycle}}
		\captionsetup{format=myformat}
		\caption[Aktivitäten im CI/CD-Prozess]{Aktivitäten im CI/CD-Prozess. In Anlehnung an Synopsys \cite{.20230201}.}
		\label{fig:CICD_Cycle}
	\end{figure}
\end{center}
\vspace*{-15mm}
Der \acs{CI}-Prozess (Continuous-Integration-Prozess) bezweckt, dass lokale Quellcode\-änderungen in kurzen Intervallen und so schnell wie möglich in eine zentrale Codebasis geladen werden. Das frühzeitige Integrieren von Code soll dabei zu einer unmittelbaren und zuverlässigen Fehlererkennung innerhalb des Entwicklungsvorhabens beitragen \cite[471]{Zampetti.92720211012021}. 
Der erste Schritt des CI-Prozesses umfasst die Planung zu entwickelnder Services (\textit{Plan}). Dabei soll festgestellt werden, welche Anforderungen eine Lösung besitzt bzw. welche Softwarearchitekturen sowie Sicherheitsmaßnahmen implementiert werden sollten. Um sicherzustellen, dass die in der Planung entworfene Anwendungsarchitektur auf das Design des Produktivsystems abgestimmt ist, sollte zu jedem Zeitpunkt das Know-how der Betriebsteams einbezogen werden \cite[16]{Halstenberg.2020}. Nach erfolgreichem Entwurf zu implementierender Anwendungsfunktionen beginnt die Entwicklung der IT-Services (\textit{Code}). Arbeiten hierbei mehrere Entwickler parallel an demselben IT-Service, wird der entsprechende Quellcode in Versionsverwaltungssysteme sog. \textit{Repositories} wie Github oder Bitbucket ausgelagert.
Ein Repository stellt dabei einen zentralen Speicherort dar, welcher das Verfolgen sowie Überprüfen von Änderungen und ein paralleles bzw. konkurrierendes Arbeiten an einer gemeinsamen Codebasis ermöglicht \cite[1]{Blischak.2016}. Der in dem Repository archivierte Hauptzweig (\textit{Master-Branch}) enthält eine aktuelle und funktionsfähige Version des Codes. Dieser mit verschiedenen Validierungsprozessen überprüfte Code stellt dabei die aktuelle in dem Produktionssystem laufende Anwendungsversion dar (s. Abb. \ref{fig:VCS}) \cite[337]{Schmiedmayer.52220225242022}.
\begin{center}
	\begin{figure}[H]\hspace*{-9mm}
		\centering
		\scalebox{0.9}{\includegraphics{VCS}}
		\caption[Integration der CI/CD-Pipeline mit dem Versionskontrollsystem]{Integration der CI/CD-Pipeline mit dem Versionskontrollsystem. Eigene Darstellung.}
		\label{fig:VCS}
	\end{figure}
\end{center}
\vspace*{-15mm}
Im Sinne der agilen Entwicklung werden große Softwareanforderungen (\textit{Epics}), in kleine Funktionalitäten (\textit{User Stories}) segmentiert, welche in separate Feature-Bran\-ches des Repositories ausgelagert werden. Diese sind unabhängige Kopien des Haupt\-zweiges, in welcher ein Entwickler Änderungen vornehmen kann, ohne Konflikte in der gemeinsamen Code-Basis zu verursachen. Nach Fertigstellung der Funktionalitäten sollte der um die Features erweiterte Quellcode so schnell wie möglich in den Hauptzweig integriert werden \cite[337]{Schmiedmayer.52220225242022}. Die Einbindung des Feature-Branches in den Hauptzweig resultiert i.d.R. in einem unmittelbaren und automatisierten Start des \textbf{CI/CD-Pipeline-Prozesses}. Bei der CI/CD-Pipeline handelt es sich dabei um ein vom Repository unabhängiges Bereitstellungsautomati\-sierungs-Tool, welches auf einer virtuellen Maschine oder in einer containerisierten Computing-Umgebung betrieben wird \cite{Codefresh.20230419}. Im ersten Schritt des Pipeline-Prozesses wird die Applikationen zu einem ausführ\-baren Programm, dem sog. \textit{Artefakt}, kompiliert (\textit{Build}). Dafür können je nach Programmiersprache verschiedene Build-Tools, wie NPM für JavaScript oder Make für \ac{MTA} verwendet werden \cite[737]{Lange.2023}. Nach Ablauf der Build-Workflows werden von der CI/CD-Pipeline verschiedene Validierungen abgewickelt (\textit{Test}). Damit soll sichergestellt werden, dass zu jeder Zeit ein rudimentär getesteter Code bereitsteht und grundlegende Funktionalitäten sowie Schnittstellen erwartungsgemäß ausgeführt werden \cite[19]{Halstenberg.2020}. Die in diesem Schritt abgewickelten Tests leiten sich dabei aus der \textit{\ac{DoD}} ab. Die DoD ist eine in der Planungsphase festgelegte Anforderungsspezifikation, deren Erfüllung als notwendige Voraussetzung für den Abschluss eines Features gilt. Somit sind Entwickler dazu angehalten, für jedes implementierte Feature einen der DoD entsprechenden Tests zu entwerfen (\textit{Test Driven Development}) \cite{.20230419}. Der in der CI-Pipeline bereitgestellte Code wird dabei überwiegend anhand schnell durchführbarer Tests überprüft. Der Zweck dieser zügigen Validierungen liegt dabei insbesondere darin, dass Entwickler zeitnahes Feedback auf die Erweiterungen erhalten. So können Fehler und Konflikte so schnell wie möglich entdeckt und behoben werden, was die Entwicklung bei einer reibungslosen Auslieferung der IT-Services unterstützt. Die in der CI-Pipeline abgewickelten Validierungen umfassen i.d.R. \textit{Unit-} sowie \textit{Integration-Tests}. Unit-Tests befinden sich dabei auf unterster Hierarchieebene der Test-Pyramide (s. Abb. \ref{fig:Tests}).
Somit besitzen diese eine kurze Ausführungsdauer, werden jedoch ausschließlich in einer isolierten Testumgebung abgewickelt. Mit Unit-Tests wird die funktionale Korrektheit kleinster Einheiten, wie z.B. Methoden einer Klasse, über\-prüft \cite[177]{Jamil.1122201611242016}. Der Zweck der Unit-Tests besteht dabei in einer von externen Einflüssen und Daten unabhängigen Überprüfung der einzelnen Komponenten.
Um bei der Bereitstellung neuer Funktionalitäten ebenfalls das Zusammenspiel verschiedener Komponenten zu überprüfen, werden \textit{Integration-Tests} durchgeführt \cite[177]{Jamil.1122201611242016}. Bei diesen Tests können Aspekte, wie der Austausch eines Nachrichtenmodells zweier Web-Services oder das Response-Objekt einer Datenbankabfrage untersucht werden. 
\begin{center}
	\begin{figure}[H]
		\centering
		\scalebox{0.6}{\includegraphics{Tests}}
		\caption[Hierarchische Darstellung von Softwaretests]{Hierarchische Darstellung von Softwaretests.\\ \hspace{0.5cm}In Anlehnung an Paspelava \cite{Exposit.2021}.}
		\label{fig:Tests}
	\end{figure}
\end{center}
\vspace*{-15mm}
Im CI-Prozess werden i.d.R. auch einfache Code-Analysen durchgeführt. Diese sollen dem Entwickler eine schnelle Rückmeldung bezüglich Verletzung von Qualitätsstandards, potenziellen Schwachstellen sowie Leistungsproblemen liefern. Nachdem alle Tests erfolgreich absolviert wurden, kann sichergestellt werden, dass der neue Quellcode stabil, also funktionsfähig ist und keine Konflikte mit dem aktuellen Code des Hauptzweiges aufweist. Somit werden alle validierten Änderungen automatisch in dem Hauptzweig zusammengeführt \cite[S. 19 ff.]{Halstenberg.2020}. Mit diesem Prozessschritt beginnt der \textbf{Continuous-Delivery-Workflow (\acs{CD}-Workflow)}.\\ Während CI den Prozess der kontinuierlichen Integration des Quellcodes in das zentrale Repository verwaltet, steuert der CD-Workflow die Automatisierung der Anwendungsbereitstellung. Applikationen sollen somit ohne große Verzögerungen in die Produktivumgebung und somit zum Kunden ausgeliefert werden. Im Sinne des DevOps-Rahmenwerkes wird der CD-Prozess automatisch und unmittelbar nach Ablauf aller CI-Aktivitäten angestoßen. In der Praxis wird hierbei jedoch häufig ein manueller Schritt zwischengeschalten (\textit{Release}) \cite[S. 19 ff.]{Halstenberg.2020}. Damit soll sichergestellt werden, dass das Ausrollen der Anwendung erst nach Überprüfung und Genehmigung der Product Owner beginnt. Im ersten Schritt des CD-Prozesses wird das in die Produktivumgebung bereitzustellende Artefakt über die Deployment-Pipeline in eine \textit{Staging-Area} geladen. Bei der Staging-Area handelt es sich dabei um ein System, welches zwischen Entwicklungs- und Produktivumgebung liegt. Die Staging-Systemkonfigurationen werden dabei so angelegt, dass diese der Produktionsumgebung möglichst ähnlich sind \cite{Reynolds.20220203}. Neben Datenbanken werden hierbei ebenfalls Serverkonfigurationen, wie Firewall- oder Netzwerkeinstellungen von dem Produktivsystem übernommen. Somit soll sichergestellt werden, dass eine neue Anwendungsversion unter produktionsähnlichen Bedingungen getestet wird. Analog zum CI-Prozess werden innerhalb des CD-Workflows ebenfalls Unit- und Integration-Tests abgewickelt. Im Gegensatz zur CI-Pipeline werden dabei ebenfalls rechenintensive Tests automatisiert. Somit werden im CD-Prozess essenzielle, jedoch während des Entwicklungsworkflows zu aufwendige Validierungen durchgeführt \cite[20]{Halstenberg.2020}. Darüber hinaus werden in der Staging Area ebenfalls in der Test-Pyramide (s. Abb. \ref{fig:Tests}) höher positionierte, also rechenintensivere Tests ausgeführt \cite{Bose.20230220}. Dazu gehören \textit{End-To-End-Tests (\acs{E2E-Test}s)}. Mit diesen soll sichergestellt werden, dass die Anforderungen aller Stakeholder erfüllt werden. Hierbei wird ein vollständiges Anwenderszenario von Anfang bis Ende getestet. Dabei wird i.d.R eine Benutzeroberfläche emuliert mit welcher etwa das Anmelden mit Benutzername, das Suchen eines Produktes und das Abschließen einer Bestellung validiert werden kann \cite{Bose.20230220}. Für kritische Systeme werden während des Delivery-Prozesses ebenfalls \textit{Regression-Tests} vorgenommen. Diese umfassen ein erneutes Testen bereits ausgelieferter Softwarekomponenten \cite[S. 15 ff.]{Engstrom.2010}. Regression-Tests können dabei in Form von Unit-, Integration- sowie E2E-Tests ausgeführt werden. Somit soll sichergestellt werden, dass sich Quellcodeänderungen nicht negativ auf die stabile Anwendungsversion auswirken. Auf oberster Ebene der Test-Pyramide befinden sich die \textit{Manual-Tests}. Dabei handelt es sich um von menschlichen Testern ausgeführte Validierungen, mit welchen Benutzerfreundlichkeit sowie Funktionalität anhand authentischer Anwenderszenarien gewährleistet werden soll \cite{Guru99.2020}. Da diese Tests nicht automatisiert durchgeführt werden können, muss der nächste Schritt der CD-Pipeline nach erfolgreicher Validierung manuell angestoßen werden. Im Anschluss an die funktionalen Tests werden i.d.R. verschiedene Code-Analysen abgewickelt. Hierbei werden Metriken, wie die prozentuale Testabdeckung oder Sch\-wachstellen verwendeter Code-Patterns überprüft \cite[146]{Rangnau.10520201082020}. Nach Durchführung der Code-Analysen wird das überprüfte Artefakt auf das Produktionssystem der Cloud-Plattform geladen (\textit{Deploy}). Je nach Bereitstellungsstrategie (s. \ref{sec:Bereitstellungs_Strategien}), wird die Anwendung dann unmittelbar oder erst nach weiteren Überprüfungen für den Kunden zugänglich gemacht.\\ Der letzte Schritt des CD-Workflows umfasst die Überwachung der inbetriebgenommenen Anwendung (\textit{Monitoring}). Diese wird i.d.R. durch ein unabhängiges Überwachungssystem und nicht von dem Pipeline-Tool selbst abgewickelt. Dabei können z.B. Dashboards zur Analyse der Build-, Test- und Deployment-Prozesse visualisiert werden. Darüber hinaus umfasst dieses Tool essenzielle Überwachungsele\-mente zum \textit{Infrastruktur-}, \textit{Plattform-} sowie \textit{Anwendungs-Monitoring} \cite{VMware.2022} \cite{Datadog.2021}\cite{.2023}. Beim Infra\-struktur-Monitoring werden Metriken wie CPU-, Speicher- und Netz\-werklast der Server bzw. Datenbanken untersucht. Das Plattform-Monitoring setzt dabei eine Ebene höher an und validiert, dass Komponenten wie Datenbanken, virtuelle Netze bzw. Middlewares ordnungsgemäß durchgeführt werden. Die auf der Plattform betriebenen Dienste können anhand des Anwendungs-Monitorings überwacht werden. Dabei werden verschiedene Aspekte wie Antwortzeiten, Fehler- und Ausfallraten bzw. Nutzungsstatistiken evaluiert.\\ 
Zur Automatisierung der CI/CD-Prozesse werden von dem SAP DTS i.d.R. drei verschiedene Tools vorgeschlagen. Eine unmittelbare von der SAP bereitgestellte Lösung ist das \textit{\ac{SAP CI/CD}}. Das SAP CI/CD ist eine auf der SAP BTP betriebene SaaS-Lösung, mit welcher vordefinierte Pipeline-Templates konfiguriert und ausgeführt werden können. Dieses Tool ist insbesondere mit SAP-Standardtechnologien, wie den Programmierframeworks SAP UI5 und SAP CAP Node sowie der Laufzeitumgebung Cloud Foundry kompatibel \cite{.20230405}. Eine weitere von dem SAP DTS empfohlene CI/CD-Alternative ist das Open-Source-Tool \textit{Jenkins}. Im Gegensatz zum templatebasierten SAP CI/CD, muss der Bereitstellungsworkflow bei Jenkins mit der Programmiersprache Groovy implementiert werden. Weiterhin wird Jenkins nicht unmittelbar auf der SAP BTP betrieben, sondern muss auf einem eigenen Server (On-Premise) verwaltet werden \cite[266]{Belmont.2018}. Um die Bereitstellung von SAP-spezifischen Technologien zu optimieren, wurde die Programmbibliothek \textit{Project Piper} verföffentlicht. In Project Piper werden vorimplementierte Pipeline-Schritte gebündelt, welche zur Erstellung einer standardisierten Pipeline verwendet werden können. Im Gegensatz zu den bei SAP CI/CD bereitgestellten Templates sind diese jedoch hoch konfigurierbar. Für jeden Pipeline-Schritt der Programmbibliothek Project Piper werden ebenfalls Docker-Container bereitgestellt, wodurch Treiber, welche z.B. für Test-Frameworks oder Build-Tools erforderlich sind, ausgeliefert werden.
Ein externes, ebenfalls von dem SAP DTS vorgeschlagenes, CI/CD-Werkzeug ist \textit{Azure Pipelines}. Azure Pipelines ist ein von Microsoft entwickeltes Tool, welches umfassende Integrationsmöglichkeiten zu anderen Microsoft-Diensten wie die Azure-Cloud-Platform oder Microsoft Visual Studio Code bietet. Zur Implementierung der CI/CD-Workflows für SAP-spezifische Technologien, wird in Azure Pipelines ebenfalls die Programmbibliothek Project Piper verwendet \cite{Naveen.20230224}.
\subsubsection{Strategien zur Bereitstellung von Neuentwicklungen}
\label{sec:Bereitstellungs_Strategien}
Nachdem das Artefakt auf einer Cloud-Instanz installiert und gestartet wurde, erfolgt die Inbetriebnahme der neuen Anwendungsversion je nach Bereitstellungsstrategie unmittelbar oder erst nach weiteren Validierungen. Anhand der Bereitstellungsstrategie wird festgelegt, mit welcher Methode IT-Services in die Produktionsumgebung bereitgestellt und zu welchem Zeitpunkt Nutzeranfragen von der aktuellen auf die neue Anwendungsinstanz umgeleitet werden (s. Abb. \ref{fig:DS}).\\
Eine häufig verwendete Deployment-Strategie ist dabei das \textit{Blue/Green-Deployment}. Hierbei wird neben der stabilen aktuellen Anwendung (\textit{Blaue Version}) ebenfalls eine Instanz des neuen IT-Services (\textit{Grüne Version}) in dem Produktionssystem betrieben. Das impliziert, dass der alte IT-Service nicht unmittelbar, sondern erst nach verschiedenen im Produktionssystem durchgeführten Validierungen durch die aktualisierte Version ersetzt wird. Ein Vorteil dieser Vorgehensweise besteht darin, dass die neue Version nicht in einer Staging-Area, sondern unmittelbar unter produktionsähnlichen Bedingungen geprüft werden kann \cite{Ugochi.20220503}. Ein zusätzlicher positiver Effekt entsteht in Zusammenhang mit der durch die Aktualisierung verursachten Ausfallzeit. Im herkömmlichen Bereitstellungsverfahren wird die alte Anwendungsversion gestoppt, der neue Service installiert und dann gestartet. Dadurch kann der Dienst erst nach erfolgreicher Inbetriebnahme der neuen Instanz wiederverwendet werden, was i.d.R. zu längeren Ausfallzeiten führt. Dies kann mithilfe des Blue/Green-Deployments vermieden werden. Hierbei wird die neue Anwendungsversion  auf einer separaten Instanz installiert. Sobald die neue Version erfolgreich gestartet ist, kann der Datenverkehr von dem Lastverteilungsservice (\textit{Load-Balancer}) unmittelbar auf diese umgeleitet werden. Somit können längere Ausfallzeiten umgangen werden \cite[1083]{Rudrabhatla.10720201092020}. 
\begin{center}
	\begin{figure}[H]
		\centering
		\scalebox{0.5}{\includegraphics{Deployment_Strategies}}
		\caption[Strategien zur Bereitstellung von Software]{Strategien zur Bereitstellung von Software.\\ In Anlehnung an Ugochi \cite{Ugochi.20220503}.}
		\label{fig:DS}
	\end{figure}
\end{center}
\vspace*{-15mm}
Im Vergleich zum Blue/Green-Deployment, bei welchem eine neue Version simultan für die gesamte Nutzerbasis zur Verfügung gestellt wird, gewährleistet das \textit{Canary-Deployment} eine restriktivere Nutzlastumleitung. Hierfür wird die neue Anwendungsversion vorerst einer überschaubaren Nutzeranzahl (\textit{Canary-Gruppe}) bereitgestellt. Dabei sollte die zusammengestellte Canary-Gruppe die Gesamtnutzerbasis möglichst gut repräsentieren. Anhand des Canary-Traffics soll der fehlerfreie Betrieb neuer Anwendungen überprüft werden. Bevor die neue Anwendungsversion der gesamten Nutzerbasis zur Verfügung gestellt wird, kann diese sukzessive und schrittweise an eine zunehmende Anwenderanzahl ausgerollt werden \cite{Ugochi.20220503}. Dies ermöglicht eine umfassende Überwachung und Kontrolle bezüglich potenziell im Produktionssystem auftretender Probleme.\\ Eine aufwendigere, jedoch risikoärmere Bereitstellungsstrategie stellt das \textit{Shadow-Deployment} dar. Dabei wird neben der Instanz der aktuellen Version ebenfalls ein sog. \textit{Shadow-Model} auf der Infrastruktur betrieben. Das Shadow-Model verwaltet die neue Version der Anwendung, kann jedoch nicht unmittelbar von den Nutzern aufgerufen werden. Diese Instanz stellt ein hinter der stabilen Version gelagertes Schattenmodell dar. Benutzeranfragen werden von dem Load-Balancer stets an die aktuelle Version der Instanz übermittelt und von dieser beantwortet. Gleichzeitig wird eine Kopie dieser Anfrage an das Shadow-Model weitergeleitet und von diesem prozessiert. Die Shadow-Modell-Verarbeitung des in der Produktionsumgebung abgewickelten Netzwerkverkehrs ermöglicht den Entwicklern somit eine anwendungsbezogene Überprüfung entwickelter Features \cite{Ugochi.20220503}.\\ Unabhängig vom Zeitpunkt, zu welchem ein IT-Service für die Nutzer bereitgestellt 
  wird, erfolgt der Anwendungsbetrieb dabei entweder im \textit{Single-} bzw. \textit{Multi-Cloud-Deployment}. Im Single-Cloud-Deployment wird eine Anwendung stets auf einer einzigen Cloud-Plattform betrieben. Im Gegensatz dazu wird das \textit{Multi-Cloud-Deployment} verwendet, um ERP-Dienste in einer verteilten Computing-Umgebung auszuführen. Dabei werden die IT-Services eines Unternehmens i.d.R. auf verschiedenen Cloud-Plattformen bereitgestellt. Der Vorteil dieser Bereitstellungsstrategie besteht darin, dass durch diese Diversifizierung verschiedene Technologien verwendet und somit eine höhere Skalierbarkeit, Verfügbarkeit und Performance der Anwendung erreicht werden kann \cite{.20230419b}.